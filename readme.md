# DP-203T00: Azure Data Engineering Zero to Hero

Si estás buscando un curso completo, práctico y aplicado para preparar la certificación de DP-203 Azure Data Engineer Associate, has venido al lugar correcto.
Este curso está diseñado para aprender todo lo relacionado con las tecnologías de ingeniería de datos con Azure como: Azure Databricks, Data Factory, Data Lake, Azure Synapse Analytics, Cosmos DB, Azure Storage y las bases de datos de Azure.

## Descripción general del laboratorio

Microsoft Azure Data Engineering es una de las profesiones de más rápido crecimiento y demanda entre los profesionales de la ciencia de datos. Según un informe de Gartner hubo un crecimiento interanual del 88 % en las ofertas de trabajo para ingenieros de datos, siendo la tasa de crecimiento más alta entre todos los trabajos de tecnología. Por ello, el certificarte en el DP-203 Azure Data Engineer Associate te dará numerosas oportunidades laborales y profesionales.

Para ello, te guiaremos a través de las competencias de Azure, compartiendo explicaciones claras y útiles consejos profesionales. Este curso está diseñado para preparar todas las habilidades específicas de la certificación DP-203 Azure Data Engineer Associate. Con la formación teórica, las guías de estudio descargables y los ejercicios prácticos.

### Formación práctica y laboratorios:
Con la formación teórica, las guías de estudio descargables, los ejercicios prácticos, los laboratorios aplicados - este es el único curso que necesitarás para prepararte.

#### [Módulo 01: Explore las opciones de cómputo y almacenamiento para las cargas de trabajo de ingeniería de datos](Instructions/Labs/LAB_01_compute_and_storage_options.md)

## Qué aprenderás:
Introducción al entorno de Azure. Aprenderas los funcamentos de Azure, ha desplegar servicios y a gestionarlos:

- Azure Data Factory. Aprenderás todo lo relacionado con Data Factory, desde Data Flows, triggers, parametrización, monitorización, etc.

- Azure Databricks. Aprenderas todo lo relacionado con Databricks, desde la ejecución y generación de notebooks, hasta la integración en DWH modernos.

- Cómo integrar Azure Data Factory y Databricks. Cubriremos todo el proceso de integración de Azure Data Factory con Databricks, desde la autenticación, transformación de datos con Databricks, visualización, etc.

- Azure Data Lakes. aprenderás que es un Data Lake, y a como desarrollarlo y utilizarlo.

- Azure Synapse Analytics. aprenderás todo lo relacionado con Azure Synapse Analytics.

- Almacenamiento con Azure Storage, Cosmos DB y Azure Databases. Aprenderos acerca del almacenamiento de datos en Azure con Cosmos DB, Azure Storage y las bases de datos de Azure.

## ¿Para quién es este curso?
- Cualquiera que aprobar la certificación DP-203 a la primera
- Cualquiera que quiera convertirse en ingeniero de datos de Azure



#### [Módulo 02: Ejecute consultas interactivas con grupos de SQL sin servidor de Azure Synapse Analytics](Instructions/Labs/LAB_02_queries_using_serverless_sql_pools.md)

En este laboratorio, los estudiantes aprenderán a trabajar con archivos almacenados en el lago de datos y fuentes de archivos externas, a través de instrucciones T-SQL ejecutadas por un grupo de SQL sin servidor en Azure Synapse Analytics. Los estudiantes consultarán los archivos de Parquet almacenados en un lago de datos, así como los archivos CSV almacenados en un almacén de datos externo. A continuación, crearán grupos de seguridad de Azure Active Directory y aplicarán el acceso a los archivos en el lago de datos a través del control de acceso basado en roles (RBAC) y las listas de control de acceso (ACL).

#### [Módulo 03: Exploración y transformación de datos en Azure Databricks](Instructions/Labs/LAB_03_data_transformation_in_databricks.md)

Este laboratorio le enseña cómo usar varios métodos de Apache Spark DataFrame para explorar y transformar datos en Azure Databricks. Aprenderá a realizar métodos estándar de DataFrame para explorar y transformar datos. También aprenderá a realizar tareas más avanzadas, como eliminar datos duplicados, manipular valores de fecha/hora, cambiar el nombre de columnas y agregar datos. Aprovisionarán la tecnología de ingestión elegida y la integrarán con Stream Analytics para crear una solución que funcione con transmisión de datos.

### Mes 2

#### [Módulo 04: Explore, transforme y cargue datos en el almacén de datos usando Apache Spark](Instructions/Labs/LAB_04_data_warehouse_using_apache_spark.md)

Este laboratorio le enseña cómo explorar datos almacenados en un lago de datos, transformar los datos y cargar datos en un almacén de datos relacional. Explorará archivos Parquet y JSON y utilizará técnicas para consultar y transformar archivos JSON con estructuras jerárquicas. Luego, utilizará Apache Spark para cargar datos en el almacén de datos y unir los datos de Parquet en el lago de datos con los datos en el grupo de SQL dedicado

#### [Módulo 05: Ingerir y cargar datos en el almacén de datos](Instructions/Labs/LAB_05_load_data_into_the_data_warehouse.md)

Este laboratorio enseña a los estudiantes cómo ingerir datos en el almacén de datos a través de secuencias de comandos T-SQL y canalizaciones de integración de Synapse Analytics. El estudiante aprenderá cómo cargar datos en grupos de SQL dedicados de Synapse con PolyBase y COPY usando T-SQL. El estudiante también aprenderá a usar la administración de cargas de trabajo junto con una actividad de copia en una canalización de Azure Synapse para la ingesta de datos a escala de petabytes..

#### [Módulo 06: Transformar datos con Azure Data Factory o Azure Synapse Pipelines](Instructions/Labs/LAB_06_transform_data_with_pipelines.md)

Este laboratorio les enseña a los estudiantes cómo construir canalizaciones de integración de datos para ingerir desde múltiples fuentes de datos, transformar datos utilizando cuadernos y flujos de datos de mapeo, y realizar el movimiento de datos en uno o más receptores de datos.

### Mes 3

#### [Módulo 07: Integrar datos de Notebooks con Azure Data Factory o Azure Synapse Pipelines](Instructions/Labs/LAB_07_integrate_data_from_notebooks.md)

En el laboratorio, los estudiantes crearán un cuaderno para consultar la actividad de los usuarios y las compras que han realizado en los últimos 12 meses. Luego, agregarán el cuaderno a una canalización mediante la nueva actividad del cuaderno y ejecutarán este cuaderno después del flujo de datos de asignación como parte de su proceso de orquestación. Al configurar esto, los estudiantes implementarán parámetros para agregar contenido dinámico en el flujo de control y validar cómo se pueden usar los parámetros.

#### [Módulo 08: Seguridad de extremo a extremo con Azure Synapse Analytics](Instructions/Labs/LAB_08_security_with_synapse_analytics.md)

En este laboratorio, los estudiantes aprenderán a proteger un espacio de trabajo de Synapse Analytics y su infraestructura de apoyo. El estudiante observará al administrador de SQL Active Directory, administrará reglas de firewall de IP, administrará secretos con Azure Key Vault y accederá a esos secretos a través de un servicio vinculado de Key Vault y actividades de canalización. El estudiante comprenderá cómo implementar la seguridad a nivel de columna, la seguridad a nivel de fila y el enmascaramiento dinámico de datos cuando se utilizan grupos de SQL dedicados.

#### [Módulo 09: Compatibilidad con el procesamiento analítico transaccional híbrido (HTAP) con Azure Synapse Link](Instructions/Labs/LAB_09_htap_with_azure_synapse_link.md)

Este laboratorio le enseña cómo Azure Synapse Link permite una conectividad perfecta de una cuenta de Azure Cosmos DB a un área de trabajo de Synapse. Comprenderá cómo habilitar y configurar el enlace de Synapse, luego cómo consultar el almacén analítico de Azure Cosmos DB mediante Apache Spark y SQL Serverless.

### Mes 4
#### [Módulo 10: Procesamiento de transmisiones en tiempo real con Stream Analytics](Instructions/Labs/LAB_10_stream_analytics.md)

Este laboratorio le enseña cómo procesar datos de transmisión con Azure Stream Analytics. Ingresará datos de telemetría de vehículos en Event Hubs y, luego, procesará esos datos en tiempo real mediante varias funciones de ventanas en Azure Stream Analytics. Enviará los datos a Azure Synapse Analytics. Finalmente, aprenderá a escalar el trabajo de Stream Analytics para aumentar el rendimiento.

#### [Módulo 11: Cree una solución de procesamiento de transmisiones con Event Hubs y Azure Databricks](Instructions/Labs/LAB_11_stream_with_azure_databricks.md)

Este laboratorio le enseña cómo ingerir y procesar datos de streaming a escala con Event Hubs y Spark Structured Streaming en Azure Databricks. Aprenderá las funciones y usos clave de la transmisión estructurada. Implementará ventanas deslizantes para agregar fragmentos de datos y aplicará marcas de agua para eliminar datos obsoletos. Finalmente, se conectará a Event Hubs para leer y escribir secuencias.

- **Are you a MCT?** - Have a look at our [GitHub User Guide for MCTs](https://microsoftlearning.github.io/MCT-User-Guide/).
                                                                       
## How should I use these files relative to the released MOC files?

- The instructor handbook and PowerPoints are still going to be your primary source for teaching the course content.

- These files on GitHub are designed to be used in conjunction with the student handbook, but are in GitHub as a central repository so MCTs and course authors can have a shared source for the latest lab files.

- the lab instructions for each module are found in the /Instructions/Labs folder. Each subfolder within this location refers to each module. For example, Lab01 relates to module01 etc. A README.md file exists in each folder with the lab instructions that the students will then follow.

- It will be recommended that for every delivery, trainers check GitHub for any changes that may have been made to support the latest Azure services, and get the latest files for their delivery.

- Please note that some of the images that you see in these lab instructions will not neccessarily reflect the state of the lab environment that you will be using in this course. For example, while browsing for files in a data lake, you may see adiitional folders in the images that may not exist in your environment. This is by design, and your lab instructions will still work.

## What about changes to the student handbook?

- We will review the student handbook on a quarterly basis and update through the normal MOC release channels as needed.

## How do I contribute?

- Any MCT can submit a issues to the code or content in the GitHub repro, Microsoft and the course author will triage and include content and lab code changes as needed.

## Classroom Materials

It is strongly recommended that MCTs and Partners access these materials and in turn, provide them separately to students.  Pointing students directly to GitHub to access Lab steps as part of an ongoing class will require them to access yet another UI as part of the course, contributing to a confusing experience for the student. An explanation to the student regarding why they are receiving separate Lab instructions can highlight the nature of an always-changing cloud-based interface and platform. Microsoft Learning support for accessing files on GitHub and support for navigation of the GitHub site is limited to MCTs teaching this course only.

## What are we doing?

- To support this course, we will need to make frequent updates to the course content to keep it current with the Azure services used in the course.  We are publishing the lab instructions and lab files on GitHub to allow for open contributions between the course authors and MCTs to keep the content current with changes in the Azure platform.

- We hope that this brings a sense of collaboration to the labs like we've never had before - when Azure changes and you find it first during a live delivery, go ahead and make an enhancement right in the lab source.  Help your fellow MCTs.

## How should I use these files relative to the released MOC files?

- The instructor handbook and PowerPoints are still going to be your primary source for teaching the course content.

- These files on GitHub are designed to be used in conjunction with the student handbook, but are in GitHub as a central repository so MCTs and course authors can have a shared source for the latest lab files.

- It will be recommended that for every delivery, trainers check GitHub for any changes that may have been made to support the latest Azure services, and get the latest files for their delivery.

## What about changes to the student handbook?

- We will review the student handbook on a quarterly basis and update through the normal MOC release channels as needed.

## How do I contribute?

- Any MCT can submit a pull request to the code or content in the GitHub repro, Microsoft and the course author will triage and include content and lab code changes as needed.

- You can submit bugs, changes, improvement and ideas.  Find a new Azure feature before we have?  Submit a new demo!

## Notes

### Classroom Materials

It is strongly recommended that MCTs and Partners access these materials and in turn, provide them separately to students.  Pointing students directly to GitHub to access Lab steps as part of an ongoing class will require them to access yet another UI as part of the course, contributing to a confusing experience for the student. An explanation to the student regarding why they are receiving separate Lab instructions can highlight the nature of an always-changing cloud-based interface and platform. Microsoft Learning support for accessing files on GitHub and support for navigation of the GitHub site is limited to MCTs teaching this course only.
